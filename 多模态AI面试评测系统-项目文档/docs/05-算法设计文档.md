# 算法设计文档

## 多模态智能面试评测系统

**文档版本**：v1.0  
**创建日期**：2025年12月

---

## 一、算法架构概述

### 1.1 多模态分析流程

```text
                    原始数据输入
                         │
         ┌───────────────┼───────────────┐
         │               │               │
         ▼               ▼               ▼
    ┌─────────┐     ┌─────────┐     ┌─────────┐
    │ 音频流  │     │ 视频流  │     │ 文本流  │
    └────┬────┘     └────┬────┘     └────┬────┘
         │               │               │
         ▼               ▼               ▼
    ┌─────────┐     ┌─────────┐     ┌─────────┐
    │ 预处理  │     │ 预处理  │     │ 预处理  │
    │ • 降噪  │     │ • 人脸  │     │ • 分词  │
    │ • 分帧  │     │   检测  │     │ • 清洗  │
    │ • VAD   │     │ • 裁剪  │     │         │
    └────┬────┘     └────┬────┘     └────┬────┘
         │               │               │
         ▼               ▼               ▼
    ┌─────────┐     ┌─────────┐     ┌─────────┐
    │ 特征    │     │ 特征    │     │ 特征    │
    │ 提取    │     │ 提取    │     │ 提取    │
    └────┬────┘     └────┬────┘     └────┬────┘
         │               │               │
         ▼               ▼               ▼
    ┌─────────┐     ┌─────────┐     ┌─────────┐
    │ 语音    │     │ 视觉    │     │ 文本    │
    │ 分析    │     │ 分析    │     │ 分析    │
    │ • ASR   │     │ • 表情  │     │ • NLU   │
    │ • 情感  │     │ • 眼神  │     │ • 逻辑  │
    │ • 韵律  │     │ • 姿态  │     │ • 质量  │
    └────┬────┘     └────┬────┘     └────┬────┘
         │               │               │
         └───────────────┼───────────────┘
                         │
                         ▼
                ┌─────────────────┐
                │   多模态融合    │
                │ • 特征对齐     │
                │ • 跨模态关联   │
                │ • 置信度加权   │
                └────────┬────────┘
                         │
                         ▼
                ┌─────────────────┐
                │   综合评分     │
                │ • 能力画像     │
                │ • 分数计算     │
                │ • 建议生成     │
                └─────────────────┘
```

### 1.2 技术栈选型

| 模块 | 技术方案 | 说明 |
|------|----------|------|
| 语音识别 | 讯飞语音SDK | 实时ASR，支持中英文 |
| 语音情感 | SER模型 | 基于CNN-LSTM |
| 人脸检测 | MediaPipe | 468个关键点 |
| 表情识别 | FER2013 + 自训练 | 7类基础表情 |
| 眼神追踪 | MediaPipe + 自研 | 注视点估计 |
| 文本分析 | 讯飞星火大模型 | 内容评估与追问生成 |
| 多模态融合 | Attention机制 | 跨模态特征融合 |

---

## 二、语音分析模块

### 2.1 语音识别（ASR）

采用讯飞实时语音识别API，支持流式识别。

```python
# 语音识别配置
class ASRConfig:
    APP_ID = "your_app_id"
    API_KEY = "your_api_key"
    API_SECRET = "your_api_secret"
    
    # 识别参数
    SAMPLE_RATE = 16000      # 采样率
    ENCODING = "raw"          # 编码格式
    LANGUAGE = "zh_cn"        # 语言
    DOMAIN = "iat"            # 应用领域
    VAD_EOS = 3000           # 静音检测时长(ms)
```

实时识别流程：

```python
import websocket
import json
import base64

class RealtimeASR:
    def __init__(self, config: ASRConfig):
        self.config = config
        self.results = []
        
    def process_audio_stream(self, audio_chunk: bytes) -> str:
        """处理音频流片段"""
        # 1. 音频预处理
        processed = self.preprocess(audio_chunk)
        
        # 2. 发送到讯飞API
        result = self.send_to_api(processed)
        
        # 3. 解析结果
        text = self.parse_result(result)
        self.results.append(text)
        
        return text
    
    def get_full_transcript(self) -> str:
        """获取完整转写文本"""
        return "".join(self.results)
```

### 2.2 语音情感分析

基于声学特征的情感识别模型。

情感类别：

| 情感 | 英文 | 说明 |
|------|------|------|
| 自信 | confident | 语调坚定、音量适中 |
| 平静 | calm | 语速平稳、情绪稳定 |
| 紧张 | nervous | 语速加快、音量波动 |
| 积极 | positive | 语调上扬、充满热情 |
| 消极 | negative | 语调低沉、缺乏活力 |

特征提取：

```python
import librosa
import numpy as np

class SpeechFeatureExtractor:
    def __init__(self):
        self.sample_rate = 16000
        
    def extract_features(self, audio_path: str) -> dict:
        """提取语音特征"""
        # 加载音频
        y, sr = librosa.load(audio_path, sr=self.sample_rate)
        
        features = {}
        
        # 1. MFCC特征（梅尔频率倒谱系数）
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        features['mfcc_mean'] = np.mean(mfcc, axis=1)
        features['mfcc_std'] = np.std(mfcc, axis=1)
        
        # 2. 基频(F0)特征
        f0, voiced_flag, voiced_probs = librosa.pyin(
            y, fmin=75, fmax=600
        )
        features['f0_mean'] = np.nanmean(f0)
        features['f0_std'] = np.nanstd(f0)
        
        # 3. 能量特征
        rms = librosa.feature.rms(y=y)
        features['energy_mean'] = np.mean(rms)
        features['energy_std'] = np.std(rms)
        
        # 4. 语速特征
        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
        features['tempo'] = tempo
        
        # 5. 过零率
        zcr = librosa.feature.zero_crossing_rate(y)
        features['zcr_mean'] = np.mean(zcr)
        
        return features
```

情感识别模型：

```python
import torch
import torch.nn as nn

class SpeechEmotionRecognizer(nn.Module):
    def __init__(self, input_size=128, hidden_size=256, num_classes=5):
        super().__init__()
        
        # CNN层提取局部特征
        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(2)
        
        # LSTM层捕捉时序特征
        self.lstm = nn.LSTM(
            input_size=128,
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )
        
        # 分类层
        self.fc = nn.Sequential(
            nn.Linear(hidden_size * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
        
    def forward(self, x):
        # CNN特征提取
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        
        # 调整维度用于LSTM
        x = x.permute(0, 2, 1)
        
        # LSTM时序建模
        lstm_out, _ = self.lstm(x)
        
        # 取最后时刻输出
        x = lstm_out[:, -1, :]
        
        # 分类
        x = self.fc(x)
        return x
```

### 2.3 语速与流畅度分析

```python
class FluencyAnalyzer:
    def __init__(self):
        self.filler_words = [
            "嗯", "啊", "那个", "就是", "然后", 
            "这个", "对吧", "其实", "可能"
        ]
        
    def analyze(self, transcript: str, duration: float) -> dict:
        """分析语速和流畅度"""
        result = {}
        
        # 1. 计算语速（字/分钟）
        char_count = len(transcript.replace(" ", ""))
        result['speech_rate'] = (char_count / duration) * 60
        result['speech_rate_level'] = self._get_rate_level(result['speech_rate'])
        
        # 2. 统计填充词
        filler_count = 0
        for word in self.filler_words:
            filler_count += transcript.count(word)
        result['filler_word_count'] = filler_count
        result['filler_word_ratio'] = filler_count / max(char_count, 1)
        
        # 3. 计算流畅度得分
        fluency_score = 100
        
        # 语速过快或过慢扣分
        if result['speech_rate'] < 120 or result['speech_rate'] > 200:
            fluency_score -= 10
            
        # 填充词过多扣分
        if result['filler_word_ratio'] > 0.05:
            fluency_score -= min(20, result['filler_word_ratio'] * 200)
            
        result['fluency_score'] = max(0, fluency_score)
        
        return result
        
    def _get_rate_level(self, rate: float) -> str:
        if rate < 120:
            return "slow"
        elif rate > 200:
            return "fast"
        else:
            return "moderate"
```

---

## 三、视频分析模块

### 3.1 人脸检测与关键点提取

使用MediaPipe进行人脸检测和468个面部关键点提取。

```python
import cv2
import mediapipe as mp
import numpy as np

class FaceDetector:
    def __init__(self):
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
    def detect(self, frame: np.ndarray) -> dict:
        """检测人脸并提取关键点"""
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.face_mesh.process(rgb_frame)
        
        if not results.multi_face_landmarks:
            return None
            
        landmarks = results.multi_face_landmarks[0]
        
        # 提取关键区域
        face_data = {
            'landmarks': self._landmarks_to_array(landmarks, frame.shape),
            'left_eye': self._get_eye_landmarks(landmarks, 'left'),
            'right_eye': self._get_eye_landmarks(landmarks, 'right'),
            'mouth': self._get_mouth_landmarks(landmarks),
            'eyebrows': self._get_eyebrow_landmarks(landmarks),
        }
        
        return face_data
        
    def _landmarks_to_array(self, landmarks, shape):
        """将关键点转换为数组"""
        h, w = shape[:2]
        points = []
        for lm in landmarks.landmark:
            points.append([lm.x * w, lm.y * h, lm.z])
        return np.array(points)
        
    def _get_eye_landmarks(self, landmarks, side):
        """获取眼睛关键点"""
        # MediaPipe眼睛关键点索引
        if side == 'left':
            indices = [33, 160, 158, 133, 153, 144]
        else:
            indices = [362, 385, 387, 263, 373, 380]
        return [landmarks.landmark[i] for i in indices]
```

### 3.2 微表情识别

基于FER2013预训练模型，针对面试场景进行微调。

表情类别映射：

| FER类别 | 面试场景解读 |
|---------|-------------|
| Happy | 自信、友好 |
| Neutral | 沉稳、专注 |
| Surprise | 思考、反应 |
| Fear | 紧张、不安 |
| Sad | 沮丧、困惑 |
| Angry | 抵触、压力 |
| Disgust | 不适、排斥 |

```python
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image

class ExpressionRecognizer:
    def __init__(self, model_path: str):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self._load_model(model_path)
        self.model.eval()
        
        self.transform = transforms.Compose([
            transforms.Resize((48, 48)),
            transforms.Grayscale(),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5])
        ])
        
        self.labels = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']
        
        # 面试场景映射
        self.interview_mapping = {
            'happy': 'confident',
            'neutral': 'calm',
            'fear': 'nervous',
            'surprise': 'thinking',
            'sad': 'uncertain',
            'angry': 'stressed',
            'disgust': 'uncomfortable'
        }
        
    def _load_model(self, model_path):
        """加载表情识别模型"""
        model = ExpressionCNN()
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        return model.to(self.device)
        
    def recognize(self, face_image: np.ndarray) -> dict:
        """识别表情"""
        # 预处理
        image = Image.fromarray(face_image)
        image = self.transform(image).unsqueeze(0).to(self.device)
        
        # 推理
        with torch.no_grad():
            outputs = self.model(image)
            probabilities = torch.softmax(outputs, dim=1)
            
        probs = probabilities.cpu().numpy()[0]
        
        # 获取主要表情
        max_idx = np.argmax(probs)
        expression = self.labels[max_idx]
        confidence = probs[max_idx]
        
        return {
            'expression': expression,
            'interview_expression': self.interview_mapping[expression],
            'confidence': float(confidence),
            'distribution': {label: float(prob) for label, prob in zip(self.labels, probs)}
        }


class ExpressionCNN(nn.Module):
    """表情识别CNN模型"""
    def __init__(self, num_classes=7):
        super().__init__()
        
        self.features = nn.Sequential(
            nn.Conv2d(1, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2),
            
            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(256 * 6 * 6, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
        )
        
    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x
```

### 3.3 眼神追踪

估计注视方向，计算眼神接触比例。

```python
class EyeGazeTracker:
    def __init__(self):
        self.mp_face_mesh = mp.solutions.face_mesh
        
        # 眼睛关键点索引
        self.LEFT_EYE = [33, 160, 158, 133, 153, 144]
        self.RIGHT_EYE = [362, 385, 387, 263, 373, 380]
        self.LEFT_IRIS = [468, 469, 470, 471, 472]
        self.RIGHT_IRIS = [473, 474, 475, 476, 477]
        
    def calculate_gaze(self, landmarks: np.ndarray, frame_shape: tuple) -> dict:
        """计算注视方向"""
        h, w = frame_shape[:2]
        
        # 获取虹膜中心
        left_iris_center = self._get_center(landmarks, self.LEFT_IRIS)
        right_iris_center = self._get_center(landmarks, self.RIGHT_IRIS)
        
        # 获取眼睛边界
        left_eye_bounds = self._get_eye_bounds(landmarks, self.LEFT_EYE)
        right_eye_bounds = self._get_eye_bounds(landmarks, self.RIGHT_EYE)
        
        # 计算水平和垂直偏移比例
        left_h_ratio = self._calc_horizontal_ratio(left_iris_center, left_eye_bounds)
        right_h_ratio = self._calc_horizontal_ratio(right_iris_center, right_eye_bounds)
        
        # 平均水平比例（0.5为正视摄像头）
        h_ratio = (left_h_ratio + right_h_ratio) / 2
        
        # 判断是否在看摄像头
        is_looking_at_camera = 0.35 < h_ratio < 0.65
        
        return {
            'horizontal_ratio': h_ratio,
            'is_looking_at_camera': is_looking_at_camera,
            'gaze_direction': self._get_direction(h_ratio)
        }
        
    def _get_center(self, landmarks, indices):
        points = landmarks[indices]
        return np.mean(points[:, :2], axis=0)
        
    def _get_eye_bounds(self, landmarks, indices):
        points = landmarks[indices]
        return {
            'left': np.min(points[:, 0]),
            'right': np.max(points[:, 0]),
            'top': np.min(points[:, 1]),
            'bottom': np.max(points[:, 1])
        }
        
    def _calc_horizontal_ratio(self, iris_center, bounds):
        return (iris_center[0] - bounds['left']) / (bounds['right'] - bounds['left'])
        
    def _get_direction(self, h_ratio):
        if h_ratio < 0.35:
            return 'left'
        elif h_ratio > 0.65:
            return 'right'
        else:
            return 'center'


class EyeContactAnalyzer:
    def __init__(self):
        self.gaze_tracker = EyeGazeTracker()
        self.looking_history = []
        
    def analyze_video(self, video_frames: list, landmarks_list: list) -> dict:
        """分析整段视频的眼神接触"""
        for frame, landmarks in zip(video_frames, landmarks_list):
            if landmarks is not None:
                gaze = self.gaze_tracker.calculate_gaze(
                    landmarks, frame.shape
                )
                self.looking_history.append(gaze['is_looking_at_camera'])
                
        # 计算眼神接触比例
        if not self.looking_history:
            return {'eye_contact_ratio': 0, 'score': 0}
            
        contact_ratio = sum(self.looking_history) / len(self.looking_history)
        
        # 评分（0.5-0.8之间最佳）
        if 0.5 <= contact_ratio <= 0.8:
            score = 100
        elif contact_ratio < 0.5:
            score = contact_ratio / 0.5 * 100
        else:
            score = max(60, 100 - (contact_ratio - 0.8) * 200)
            
        return {
            'eye_contact_ratio': contact_ratio,
            'score': score,
            'feedback': self._get_feedback(contact_ratio)
        }
        
    def _get_feedback(self, ratio):
        if ratio < 0.3:
            return "眼神交流不足，建议增加与镜头的对视"
        elif ratio > 0.9:
            return "眼神过于固定，建议自然一些"
        else:
            return "眼神交流良好"
```

### 3.4 肢体语言分析

```python
class PostureAnalyzer:
    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
    def analyze(self, frame: np.ndarray) -> dict:
        """分析姿态"""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = self.pose.process(rgb)
        
        if not results.pose_landmarks:
            return None
            
        landmarks = results.pose_landmarks.landmark
        
        # 计算肩膀倾斜度
        shoulder_tilt = self._calc_shoulder_tilt(landmarks)
        
        # 计算头部倾斜度
        head_tilt = self._calc_head_tilt(landmarks)
        
        # 综合姿态得分
        score = 100
        if abs(shoulder_tilt) > 10:
            score -= min(20, abs(shoulder_tilt))
        if abs(head_tilt) > 15:
            score -= min(15, abs(head_tilt) - 5)
            
        return {
            'shoulder_tilt': shoulder_tilt,
            'head_tilt': head_tilt,
            'posture_score': max(0, score),
            'feedback': self._get_feedback(shoulder_tilt, head_tilt)
        }
        
    def _calc_shoulder_tilt(self, landmarks):
        """计算肩膀倾斜角度"""
        left_shoulder = landmarks[self.mp_pose.PoseLandmark.LEFT_SHOULDER]
        right_shoulder = landmarks[self.mp_pose.PoseLandmark.RIGHT_SHOULDER]
        
        dy = left_shoulder.y - right_shoulder.y
        dx = left_shoulder.x - right_shoulder.x
        
        angle = np.degrees(np.arctan2(dy, dx))
        return angle
        
    def _calc_head_tilt(self, landmarks):
        """计算头部倾斜角度"""
        nose = landmarks[self.mp_pose.PoseLandmark.NOSE]
        left_ear = landmarks[self.mp_pose.PoseLandmark.LEFT_EAR]
        right_ear = landmarks[self.mp_pose.PoseLandmark.RIGHT_EAR]
        
        # 耳朵中点
        ear_mid_y = (left_ear.y + right_ear.y) / 2
        dy = nose.y - ear_mid_y
        
        return np.degrees(np.arcsin(dy)) if abs(dy) < 1 else 0
        
    def _get_feedback(self, shoulder_tilt, head_tilt):
        feedbacks = []
        if abs(shoulder_tilt) > 10:
            feedbacks.append("注意保持肩膀平正")
        if abs(head_tilt) > 15:
            feedbacks.append("注意保持头部端正")
        return feedbacks if feedbacks else ["姿态良好"]
```

---

## 四、文本分析模块

### 4.1 大模型内容评估

使用讯飞星火大模型进行内容分析。

```python
import requests
import json

class ContentAnalyzer:
    def __init__(self, api_config: dict):
        self.api_url = api_config['api_url']
        self.api_key = api_config['api_key']
        self.model = api_config.get('model', 'spark-v3.5')
        
    def analyze(self, question: str, answer: str, job_requirements: list) -> dict:
        """分析回答内容"""
        
        prompt = self._build_analysis_prompt(question, answer, job_requirements)
        
        response = self._call_api(prompt)
        
        # 解析大模型返回的评估结果
        result = self._parse_response(response)
        
        return result
        
    def _build_analysis_prompt(self, question: str, answer: str, requirements: list) -> str:
        return f"""你是一位专业的面试评估专家，请对以下面试回答进行评估。

【面试问题】
{question}

【候选人回答】
{answer}

【岗位要求】
{chr(10).join(f'- {req}' for req in requirements)}

请从以下维度进行评估，并给出0-100的分数：

1. 内容相关度：回答是否切题，是否回答了问题的核心
2. 内容深度：是否有具体案例、数据支撑，专业程度如何
3. 逻辑结构：回答是否有条理，是否采用了STAR等结构化表达
4. 专业匹配：与岗位要求的匹配程度
5. 创新亮点：是否有独特见解或创新思维

请以JSON格式返回评估结果：
{{
    "content_relevance": {{
        "score": 分数,
        "comment": "评语"
    }},
    "content_depth": {{
        "score": 分数,
        "comment": "评语"
    }},
    "logic_structure": {{
        "score": 分数,
        "structure_type": "识别的结构类型",
        "comment": "评语"
    }},
    "professional_match": {{
        "score": 分数,
        "matched_requirements": ["匹配的要求"],
        "comment": "评语"
    }},
    "highlights": ["亮点1", "亮点2"],
    "improvements": ["改进建议1", "改进建议2"],
    "overall_score": 综合分数,
    "overall_comment": "总体评价"
}}"""
        
    def _call_api(self, prompt: str) -> str:
        """调用大模型API"""
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': self.model,
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
            'temperature': 0.3,
            'max_tokens': 2000
        }
        
        response = requests.post(
            self.api_url,
            headers=headers,
            json=payload
        )
        
        return response.json()['choices'][0]['message']['content']
        
    def _parse_response(self, response: str) -> dict:
        """解析大模型响应"""
        try:
            # 提取JSON部分
            start = response.find('{')
            end = response.rfind('}') + 1
            json_str = response[start:end]
            return json.loads(json_str)
        except:
            return {'error': 'Failed to parse response', 'raw': response}
```

### 4.2 动态追问生成

```python
class FollowUpGenerator:
    def __init__(self, api_config: dict):
        self.api_url = api_config['api_url']
        self.api_key = api_config['api_key']
        
    def generate(self, question: str, answer: str, analysis: dict, style: str = 'friendly') -> str:
        """生成追问问题"""
        
        prompt = self._build_followup_prompt(question, answer, analysis, style)
        response = self._call_api(prompt)
        
        return response
        
    def _build_followup_prompt(self, question: str, answer: str, analysis: dict, style: str) -> str:
        style_description = {
            'friendly': '友好亲切，鼓励引导',
            'serious': '专业严谨，注重细节',
            'pressure': '具有挑战性，测试抗压能力'
        }
        
        return f"""你是一位{style_description.get(style, '专业')}的面试官。

【原问题】
{question}

【候选人回答】
{answer}

【回答分析】
- 亮点：{', '.join(analysis.get('highlights', []))}
- 待深入：{', '.join(analysis.get('improvements', []))}

请生成1个追问问题，要求：
1. 针对回答中可以深入的点进行追问
2. 符合{style_description.get(style, '专业')}的风格
3. 问题简洁明了，不超过50字
4. 能够进一步考察候选人的能力

只返回追问问题，不要其他内容。"""
        
    def _call_api(self, prompt: str) -> str:
        headers = {
            'Authorization': f'Bearer {self.api_key}',
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': 'spark-v3.5',
            'messages': [
                {'role': 'user', 'content': prompt}
            ],
            'temperature': 0.7,
            'max_tokens': 100
        }
        
        response = requests.post(self.api_url, headers=headers, json=payload)
        return response.json()['choices'][0]['message']['content']
```

---

## 五、多模态融合模块

### 5.1 特征对齐

将不同模态的特征对齐到统一的时间轴。

```python
class FeatureAligner:
    def __init__(self, target_fps: int = 10):
        self.target_fps = target_fps
        
    def align(self, speech_features: list, video_features: list, 
              text_timestamps: list, duration: float) -> dict:
        """对齐多模态特征"""
        
        # 计算目标帧数
        num_frames = int(duration * self.target_fps)
        
        aligned = {
            'speech': self._align_speech(speech_features, num_frames),
            'video': self._align_video(video_features, num_frames),
            'text': self._align_text(text_timestamps, num_frames, duration)
        }
        
        return aligned
        
    def _align_speech(self, features, num_frames):
        """对齐语音特征"""
        if len(features) == 0:
            return [None] * num_frames
            
        # 重采样到目标帧数
        indices = np.linspace(0, len(features) - 1, num_frames).astype(int)
        return [features[i] for i in indices]
        
    def _align_video(self, features, num_frames):
        """对齐视频特征"""
        if len(features) == 0:
            return [None] * num_frames
            
        indices = np.linspace(0, len(features) - 1, num_frames).astype(int)
        return [features[i] for i in indices]
        
    def _align_text(self, timestamps, num_frames, duration):
        """对齐文本特征（基于时间戳）"""
        frame_duration = duration / num_frames
        aligned = []
        
        for i in range(num_frames):
            frame_time = i * frame_duration
            # 找到该时间点对应的文本
            text = self._find_text_at_time(timestamps, frame_time)
            aligned.append(text)
            
        return aligned
        
    def _find_text_at_time(self, timestamps, time):
        for item in timestamps:
            if item['start'] <= time < item['end']:
                return item['text']
        return None
```

### 5.2 跨模态关联分析

识别语音情感与面部表情的一致性。

```python
class CrossModalAnalyzer:
    def __init__(self):
        self.emotion_mapping = {
            'confident': ['happy', 'neutral'],
            'nervous': ['fear', 'surprise'],
            'calm': ['neutral'],
            'positive': ['happy'],
            'negative': ['sad', 'angry']
        }
        
    def analyze_consistency(self, speech_emotion: str, face_expression: str) -> dict:
        """分析语音情感与面部表情的一致性"""
        
        # 检查是否一致
        expected_expressions = self.emotion_mapping.get(speech_emotion, [])
        is_consistent = face_expression in expected_expressions
        
        # 计算一致性得分
        if is_consistent:
            consistency_score = 1.0
        else:
            # 部分相关的情况
            consistency_score = self._calc_partial_score(speech_emotion, face_expression)
            
        return {
            'is_consistent': is_consistent,
            'consistency_score': consistency_score,
            'speech_emotion': speech_emotion,
            'face_expression': face_expression,
            'analysis': self._get_analysis(is_consistent, speech_emotion, face_expression)
        }
        
    def _calc_partial_score(self, speech, face):
        """计算部分一致的得分"""
        # 情感距离矩阵（简化版）
        distance_matrix = {
            ('confident', 'neutral'): 0.8,
            ('confident', 'surprise'): 0.5,
            ('nervous', 'neutral'): 0.6,
            ('calm', 'happy'): 0.7,
        }
        return distance_matrix.get((speech, face), 0.3)
        
    def _get_analysis(self, is_consistent, speech, face):
        if is_consistent:
            return "语音情感与面部表情一致，表达真实可信"
        else:
            return f"语音显示{speech}，但面部表情为{face}，存在不一致"
```

### 5.3 综合评分模型

```python
class MultiModalFusion:
    def __init__(self, ability_weights: dict = None):
        self.default_weights = {
            'professional_knowledge': 0.25,
            'logical_thinking': 0.20,
            'language_expression': 0.20,
            'stress_resistance': 0.20,
            'communication_potential': 0.15
        }
        self.ability_weights = ability_weights or self.default_weights
        
    def fuse(self, speech_analysis: dict, video_analysis: dict, 
             text_analysis: dict) -> dict:
        """融合多模态分析结果"""
        
        # 1. 提取各模态得分
        speech_score = speech_analysis.get('overall_score', 0)
        video_score = video_analysis.get('overall_score', 0)
        text_score = text_analysis.get('overall_score', 0)
        
        # 2. 计算各能力维度得分
        ability_scores = self._calc_ability_scores(
            speech_analysis, video_analysis, text_analysis
        )
        
        # 3. 计算综合得分
        overall_score = sum(
            score * self.ability_weights.get(ability, 0)
            for ability, score in ability_scores.items()
        )
        
        # 4. 跨模态一致性调整
        consistency = self._check_consistency(speech_analysis, video_analysis)
        if consistency['score'] < 0.5:
            overall_score *= 0.95  # 不一致时轻微扣分
            
        # 5. 生成推荐结果
        recommendation = self._get_recommendation(overall_score)
        
        return {
            'overall_score': round(overall_score, 2),
            'recommendation': recommendation,
            'ability_scores': ability_scores,
            'modality_scores': {
                'speech': speech_score,
                'video': video_score,
                'text': text_score
            },
            'consistency': consistency
        }
        
    def _calc_ability_scores(self, speech, video, text) -> dict:
        """计算各能力维度得分"""
        return {
            'professional_knowledge': text.get('professional_match', {}).get('score', 0),
            'logical_thinking': (
                text.get('logic_structure', {}).get('score', 0) * 0.7 +
                speech.get('fluency_score', 0) * 0.3
            ),
            'language_expression': (
                speech.get('overall_score', 0) * 0.6 +
                text.get('content_relevance', {}).get('score', 0) * 0.4
            ),
            'stress_resistance': (
                video.get('confidence_score', 0) * 0.5 +
                speech.get('emotion_score', 0) * 0.5
            ),
            'communication_potential': (
                video.get('eye_contact_score', 0) * 0.4 +
                video.get('posture_score', 0) * 0.3 +
                speech.get('fluency_score', 0) * 0.3
            )
        }
        
    def _check_consistency(self, speech, video):
        """检查跨模态一致性"""
        speech_emotion = speech.get('emotion', 'neutral')
        face_expression = video.get('dominant_expression', 'neutral')
        
        analyzer = CrossModalAnalyzer()
        return analyzer.analyze_consistency(speech_emotion, face_expression)
        
    def _get_recommendation(self, score):
        if score >= 80:
            return 'recommend'
        elif score >= 60:
            return 'pending'
        else:
            return 'not_recommend'
```

---

## 六、评测报告生成

### 6.1 报告生成器

```python
class ReportGenerator:
    def __init__(self, api_config: dict):
        self.api_config = api_config
        
    def generate(self, interview_data: dict, fusion_result: dict) -> dict:
        """生成完整评测报告"""
        
        report = {
            'interview_id': interview_data['id'],
            'candidate': interview_data['candidate'],
            'position': interview_data['position'],
            
            # 综合评分
            'overall_score': fusion_result['overall_score'],
            'recommendation': fusion_result['recommendation'],
            
            # 能力雷达图数据
            'ability_scores': fusion_result['ability_scores'],
            
            # 各模态详细数据
            'dimension_details': self._build_dimension_details(
                interview_data['analysis_results']
            ),
            
            # 亮点时刻
            'highlights': self._extract_highlights(interview_data),
            
            # 改进建议
            'improvements': self._generate_improvements(fusion_result),
            
            # 生成时间
            'generated_at': datetime.utcnow().isoformat()
        }
        
        return report
        
    def _build_dimension_details(self, analysis_results: dict) -> dict:
        """构建各维度详细数据"""
        return {
            'speech': {
                'score': analysis_results['speech']['overall_score'],
                'speech_rate': analysis_results['speech']['speech_rate_level'],
                'fluency': self._level(analysis_results['speech']['fluency_score']),
                'emotion': analysis_results['speech']['emotion']
            },
            'video': {
                'score': analysis_results['video']['overall_score'],
                'expression': analysis_results['video']['dominant_expression'],
                'eye_contact': self._level(analysis_results['video']['eye_contact_score']),
                'posture': self._level(analysis_results['video']['posture_score'])
            },
            'content': {
                'score': analysis_results['text']['overall_score'],
                'relevance': self._level(analysis_results['text']['content_relevance']['score']),
                'logic_structure': analysis_results['text']['logic_structure']['structure_type'],
                'professional_depth': self._level(analysis_results['text']['content_depth']['score'])
            }
        }
        
    def _level(self, score: float) -> str:
        if score >= 90:
            return '优秀'
        elif score >= 75:
            return '良好'
        elif score >= 60:
            return '一般'
        else:
            return '待提升'
            
    def _extract_highlights(self, interview_data: dict) -> list:
        """提取亮点时刻"""
        highlights = []
        
        for answer in interview_data['answers']:
            if answer['analysis']['text'].get('highlights'):
                for h in answer['analysis']['text']['highlights']:
                    highlights.append({
                        'timestamp': answer['timestamp'],
                        'question_order': answer['order'],
                        'type': 'positive',
                        'description': h
                    })
                    
        return highlights
        
    def _generate_improvements(self, fusion_result: dict) -> list:
        """生成改进建议"""
        improvements = []
        
        ability_scores = fusion_result['ability_scores']
        
        # 找出得分较低的维度
        for ability, score in ability_scores.items():
            if score < 70:
                improvements.append({
                    'dimension': ability,
                    'score': score,
                    'suggestion': self._get_suggestion(ability, score)
                })
                
        return improvements
        
    def _get_suggestion(self, ability: str, score: float) -> str:
        suggestions = {
            'professional_knowledge': '建议加强专业知识学习，多准备具体项目案例',
            'logical_thinking': '建议采用STAR法则组织回答，使表达更有条理',
            'language_expression': '建议控制语速，减少语气词，提高表达流畅度',
            'stress_resistance': '建议多进行模拟面试练习，提升心理素质',
            'communication_potential': '建议增加与镜头的眼神交流，保持良好姿态'
        }
        return suggestions.get(ability, '继续努力提升')
```

---

## 七、模型训练与优化

### 7.1 表情识别模型训练

```python
# 训练配置
training_config = {
    'batch_size': 64,
    'learning_rate': 0.001,
    'epochs': 50,
    'optimizer': 'Adam',
    'scheduler': 'CosineAnnealingLR',
    'data_augmentation': [
        'RandomHorizontalFlip',
        'RandomRotation(10)',
        'RandomAffine'
    ]
}

# 数据增强
train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])
```

### 7.2 模型评估指标

| 指标 | 目标值 |
|------|--------|
| 表情识别准确率 | > 85% |
| 眼神追踪准确率 | > 90% |
| 语音情感准确率 | > 80% |
| 综合评分MAE | < 5分 |

---

文档修订记录

| 版本 | 日期 | 修订人 | 修订内容 |
|------|------|--------|----------|
| v1.0 | 2025-12 | - | 初稿创建 |
